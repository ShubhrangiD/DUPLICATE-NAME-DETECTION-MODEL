{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries"
      ],
      "metadata": {
        "id": "XBmczIojXTb1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "cAs6ppBEsseL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd  # for handling data frames.\n",
        "from jellyfish import soundex, metaphone, levenshtein_distance, jaro_winkler_similarity #for phonetic encoding and string similarity functions.\n",
        "from collections import defaultdict # for creating dictionaries with default values.\n",
        "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor #for parallel processing.\n",
        "import itertools #for efficient looping and combinations."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the Name"
      ],
      "metadata": {
        "id": "ohk87zwbifrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_name(name: str) -> str:\n",
        "    return name.strip().lower() if isinstance(name, str) else ''\n",
        "\n",
        "#strip() removes leading and trailing spaces.\n",
        "#lower() converts the string to lowercase."
      ],
      "metadata": {
        "id": "TkdfPJJLtGsd"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Similarity Score"
      ],
      "metadata": {
        "id": "P6GutyFUilI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similarity_score(name1, name2):\n",
        "    name1, name2 = preprocess_name(name1), preprocess_name(name2) #Preprocess both names.\n",
        "\n",
        "    #Phonetic codes\n",
        "    #Compute phonetic scores using soundex and metaphone.\n",
        "    soundex_score = soundex(name1) == soundex(name2)\n",
        "    metaphone_score = metaphone(name1) == metaphone(name2)\n",
        "\n",
        "    #String similarity scores\n",
        "    #Compute string similarity scores using levenshtein_distance and jaro_winkler_similarity\n",
        "    levenshtein_score = levenshtein_distance(name1, name2)\n",
        "    jaro_winkler_score = jaro_winkler_similarity(name1, name2)\n",
        "\n",
        "    #Combine these scores with weights to get a final similarity score.\n",
        "    score = (soundex_score * 0.2 +\n",
        "             metaphone_score * 0.2 +\n",
        "             (1 - levenshtein_score / max(len(name1), len(name2))) * 0.3 +\n",
        "             jaro_winkler_score * 0.3)\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "sQitfBGXtKvy"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file\n",
        "org_members_data = pd.read_csv('org_members.csv')"
      ],
      "metadata": {
        "id": "q_P1RZ8StS1y"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data: strip spaces and convert to lower case\n",
        "org_members_data['name'] = org_members_data['name'].apply(preprocess_name)"
      ],
      "metadata": {
        "id": "KGYlAryItYkT"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block Names:\n",
        "\n",
        "This function groups names by their first letter to reduce the number of comparisons"
      ],
      "metadata": {
        "id": "V5k1HvsXiyhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Blocking function based on the first letter of the name\n",
        "#Create a dictionary where keys are the first letters, and values are lists of names starting with that letter\n",
        "def block_names(df: pd.DataFrame) -> dict[str, list[str]]:\n",
        "    blocks = defaultdict(list)\n",
        "    for _, row in df.iterrows():\n",
        "        first_letter = row['name'][0] if row['name'] else ''\n",
        "        blocks[first_letter].append(row['name'])\n",
        "    return blocks"
      ],
      "metadata": {
        "id": "AoBjuicwtaE1"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process Chunk of Names:\n",
        "\n",
        "This function takes a chunk of names and compares each pair to see if they are similar based on a threshold. If they are, it records them as potential duplicates."
      ],
      "metadata": {
        "id": "wHyADvhgi07B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a function to process a chunk of names:\n",
        "#Calculate similarity scores for each pair of names.\n",
        "#If the score exceeds a threshold, add them as potential duplicates.\n",
        "#Ensure no duplicate pairs by using a set.\n",
        "def process_chunk(chunk: list[str], threshold: float = 0.8) -> dict[str, list[str]]:\n",
        "    potential_duplicates = defaultdict(list)\n",
        "    seen_pairs = set()\n",
        "\n",
        "    for name1, name2 in itertools.combinations(chunk, 2):\n",
        "        # Ensure name1 is less than name2 to avoid duplicates and unnecessary repetitions\n",
        "        if name1 > name2:\n",
        "            name1, name2 = name2, name1\n",
        "\n",
        "        pair = (name1, name2)\n",
        "\n",
        "        if pair not in seen_pairs:\n",
        "            score = get_similarity_score(name1, name2)\n",
        "            if score > threshold and score < 1.00:\n",
        "                potential_duplicates[name1].append(f\"{name2} ({score:.2f})\")\n",
        "                potential_duplicates[name2].append(f\"{name1} ({score:.2f})\")\n",
        "            seen_pairs.add(pair)\n",
        "\n",
        "    return potential_duplicates"
      ],
      "metadata": {
        "id": "Hkku65vmtyDd"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create batches based on the blocked names\n",
        "blocked_org_members = block_names(org_members_data)"
      ],
      "metadata": {
        "id": "7F9_vBIWuVQI"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process Blocks with Threads:\n",
        "\n",
        "This function processes names in parallel using multiple threads to speed up the comparison process."
      ],
      "metadata": {
        "id": "bZU3l7PHi_u9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a function to process blocks of names using threads:\n",
        "#Divide names into chunks.\n",
        "#Process each chunk in parallel using ThreadPoolExecutor.\n",
        "#Combine results from all threads.\n",
        "\n",
        "#Function to process blocks using threads\n",
        "def process_block_with_threads(names: list[str], threshold: float = 0.8, chunk_size: int = 1000) -> dict[str, list[str]]:\n",
        "    potential_duplicates = defaultdict(list)\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        futures = []\n",
        "        for i in range(0, len(names), chunk_size):\n",
        "            chunk = names[i:i + chunk_size]\n",
        "            futures.append(executor.submit(process_chunk, chunk, threshold))\n",
        "\n",
        "        for future in futures:\n",
        "            result = future.result()\n",
        "            for key, value in result.items():\n",
        "                potential_duplicates[key].extend(value)\n",
        "\n",
        "    return potential_duplicates"
      ],
      "metadata": {
        "id": "HV0xvftEuj-H"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process All Blocks in Parallel:\n",
        "\n",
        "This function processes all blocks in parallel using multiple processes to further speed up the overall computation."
      ],
      "metadata": {
        "id": "u5_3wMuWjFDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Process each block in parallel using ProcessPoolExecutor\n",
        "#Submit blocks for processing, Collect and combine results\n",
        "\n",
        "all_potential_duplicates = defaultdict(list)\n",
        "with ProcessPoolExecutor() as executor:\n",
        "    futures = []\n",
        "    for names in blocked_org_members.values():\n",
        "        futures.append(executor.submit(process_block_with_threads, names))\n",
        "\n",
        "    for future in futures:\n",
        "        result = future.result()\n",
        "        for key, value in result.items():\n",
        "            all_potential_duplicates[key].extend(value)\n"
      ],
      "metadata": {
        "id": "hxnj3zeTutFh"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consolidate Similar Names into Clusters:\n",
        "\n",
        "This part combines similar names into clusters and ensures each name is only processed once."
      ],
      "metadata": {
        "id": "9YKHhQgvjMV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Consolidate similar names into clusters\n",
        "#Create clusters of similar names.\n",
        "#Mark names as visited to avoid processing them multiple times.\n",
        "\n",
        "clusters = []\n",
        "visited = set()\n",
        "\n",
        "for key, similars in all_potential_duplicates.items():\n",
        "    if key not in visited:\n",
        "        cluster = [key] + similars\n",
        "        unique_cluster = sorted(set(cluster), key=str.lower)  # Remove duplicates and sort alphabetically\n",
        "        visited.update(preprocess_name(name.split(' (')[0]) for name in unique_cluster)  # Update with base name\n",
        "        clusters.append(unique_cluster)\n"
      ],
      "metadata": {
        "id": "cQUeQGUIuzmS"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort clusters alphabetically by name\n",
        "clusters.sort(key=lambda x: x[0])\n",
        "\n",
        "# Create DataFrame for clusters\n",
        "clustered_df = pd.DataFrame(clusters)"
      ],
      "metadata": {
        "id": "MSeuehXCu163"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename columns\n",
        "max_cols = clustered_df.shape[1]\n",
        "column_names = ['name'] + [f'duplicate_{i}' for i in range(1, max_cols)]\n",
        "clustered_df.columns = column_names"
      ],
      "metadata": {
        "id": "AYPnbBgtu7Pe"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save clusters to a CSV file\n",
        "clustered_df.to_csv('DUPLICATE_NAMES.csv', index=False)"
      ],
      "metadata": {
        "id": "Nb5CjAfPvBCi"
      },
      "execution_count": 44,
      "outputs": []
    }
  ]
}